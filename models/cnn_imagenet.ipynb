{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.applications import mobilenet\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input, decode_predictions\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.imagenet_utils import obtain_input_shape\n",
    "from keras.models import Model\n",
    "from keras.optimizers import adam_v2\n",
    "from PIL import Image\n",
    "\n",
    "from keras.layers import Dense, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.mobilenet import MobileNet\n",
    "\n",
    "base_model = MobileNet(weights='imagenet', include_top=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "preds = Dense(3, activation='softmax')(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=preds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_7\n",
      "1 conv1\n",
      "2 conv1_bn\n",
      "3 conv1_relu\n",
      "4 conv_dw_1\n",
      "5 conv_dw_1_bn\n",
      "6 conv_dw_1_relu\n",
      "7 conv_pw_1\n",
      "8 conv_pw_1_bn\n",
      "9 conv_pw_1_relu\n",
      "10 conv_pad_2\n",
      "11 conv_dw_2\n",
      "12 conv_dw_2_bn\n",
      "13 conv_dw_2_relu\n",
      "14 conv_pw_2\n",
      "15 conv_pw_2_bn\n",
      "16 conv_pw_2_relu\n",
      "17 conv_dw_3\n",
      "18 conv_dw_3_bn\n",
      "19 conv_dw_3_relu\n",
      "20 conv_pw_3\n",
      "21 conv_pw_3_bn\n",
      "22 conv_pw_3_relu\n",
      "23 conv_pad_4\n",
      "24 conv_dw_4\n",
      "25 conv_dw_4_bn\n",
      "26 conv_dw_4_relu\n",
      "27 conv_pw_4\n",
      "28 conv_pw_4_bn\n",
      "29 conv_pw_4_relu\n",
      "30 conv_dw_5\n",
      "31 conv_dw_5_bn\n",
      "32 conv_dw_5_relu\n",
      "33 conv_pw_5\n",
      "34 conv_pw_5_bn\n",
      "35 conv_pw_5_relu\n",
      "36 conv_pad_6\n",
      "37 conv_dw_6\n",
      "38 conv_dw_6_bn\n",
      "39 conv_dw_6_relu\n",
      "40 conv_pw_6\n",
      "41 conv_pw_6_bn\n",
      "42 conv_pw_6_relu\n",
      "43 conv_dw_7\n",
      "44 conv_dw_7_bn\n",
      "45 conv_dw_7_relu\n",
      "46 conv_pw_7\n",
      "47 conv_pw_7_bn\n",
      "48 conv_pw_7_relu\n",
      "49 conv_dw_8\n",
      "50 conv_dw_8_bn\n",
      "51 conv_dw_8_relu\n",
      "52 conv_pw_8\n",
      "53 conv_pw_8_bn\n",
      "54 conv_pw_8_relu\n",
      "55 conv_dw_9\n",
      "56 conv_dw_9_bn\n",
      "57 conv_dw_9_relu\n",
      "58 conv_pw_9\n",
      "59 conv_pw_9_bn\n",
      "60 conv_pw_9_relu\n",
      "61 conv_dw_10\n",
      "62 conv_dw_10_bn\n",
      "63 conv_dw_10_relu\n",
      "64 conv_pw_10\n",
      "65 conv_pw_10_bn\n",
      "66 conv_pw_10_relu\n",
      "67 conv_dw_11\n",
      "68 conv_dw_11_bn\n",
      "69 conv_dw_11_relu\n",
      "70 conv_pw_11\n",
      "71 conv_pw_11_bn\n",
      "72 conv_pw_11_relu\n",
      "73 conv_pad_12\n",
      "74 conv_dw_12\n",
      "75 conv_dw_12_bn\n",
      "76 conv_dw_12_relu\n",
      "77 conv_pw_12\n",
      "78 conv_pw_12_bn\n",
      "79 conv_pw_12_relu\n",
      "80 conv_dw_13\n",
      "81 conv_dw_13_bn\n",
      "82 conv_dw_13_relu\n",
      "83 conv_pw_13\n",
      "84 conv_pw_13_bn\n",
      "85 conv_pw_13_relu\n",
      "86 global_average_pooling2d_6\n",
      "87 dense_12\n",
      "88 dense_13\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(model.layers): print(i, layer.name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "for layer in model.layers[:86]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[86:]:\n",
    "    layer.trainable = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "# Fitting CNN to images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 images belonging to 3 classes.\n",
      "Found 24 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# target size is the mobilenet target size that is required (224,224)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('../data/fruits_cnn_data/Training',\n",
    "                                                 target_size=(224, 224),\n",
    "                                                 batch_size=8,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('../data/fruits_cnn_data/Validation',\n",
    "                                                 target_size=(224, 224),\n",
    "                                                 batch_size=8,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "\n",
    "# categorical cz 3 classes, if only 2 classes - binary o/p required then\n",
    "# classmode = 'binary'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2022-01-23 22:56:03.123853\n"
     ]
    }
   ],
   "source": [
    "model_name = 'fruitdetect_model'\n",
    "import datetime\n",
    "print(\"Start time:\", datetime.datetime.now())\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\n",
    "checkpoint_callback = ModelCheckpoint(model_name+ '.h5', monitor='val_loss', verbose=1, save_best_only=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sylvi\\AppData\\Local\\Temp\\ipykernel_3332\\582598675.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(training_set,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.5025 - accuracy: 0.5000    \n",
      "Epoch 00001: val_loss improved from inf to 0.05635, saving model to fruitdetect_model.h5\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.5025 - accuracy: 0.5000 - val_loss: 0.0563 - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 1.0000\n",
      "Epoch 00002: val_loss improved from 0.05635 to 0.00981, saving model to fruitdetect_model.h5\n",
      "2/2 [==============================] - 1s 939ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 00003: val_loss improved from 0.00981 to 0.00331, saving model to fruitdetect_model.h5\n",
      "2/2 [==============================] - 1s 771ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00004: val_loss improved from 0.00331 to 0.00168, saving model to fruitdetect_model.h5\n",
      "2/2 [==============================] - 1s 1s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 7.1809e-04 - accuracy: 1.0000\n",
      "Epoch 00005: val_loss improved from 0.00168 to 0.00085, saving model to fruitdetect_model.h5\n",
      "2/2 [==============================] - 1s 902ms/step - loss: 7.1809e-04 - accuracy: 1.0000 - val_loss: 8.5004e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6571e-04 - accuracy: 1.0000\n",
      "Epoch 00006: val_loss improved from 0.00085 to 0.00026, saving model to fruitdetect_model.h5\n",
      "2/2 [==============================] - 1s 972ms/step - loss: 3.6571e-04 - accuracy: 1.0000 - val_loss: 2.6120e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8398e-04 - accuracy: 1.0000\n",
      "Epoch 00007: val_loss improved from 0.00026 to 0.00022, saving model to fruitdetect_model.h5\n",
      "2/2 [==============================] - 1s 803ms/step - loss: 1.8398e-04 - accuracy: 1.0000 - val_loss: 2.1870e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1913e-04 - accuracy: 1.0000\n",
      "Epoch 00008: val_loss improved from 0.00022 to 0.00015, saving model to fruitdetect_model.h5\n",
      "2/2 [==============================] - 1s 840ms/step - loss: 1.1913e-04 - accuracy: 1.0000 - val_loss: 1.4975e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 8.3110e-05 - accuracy: 1.0000\n",
      "Epoch 00009: val_loss improved from 0.00015 to 0.00010, saving model to fruitdetect_model.h5\n",
      "2/2 [==============================] - 1s 802ms/step - loss: 8.3110e-05 - accuracy: 1.0000 - val_loss: 9.7890e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.9327e-05 - accuracy: 1.0000\n",
      "Epoch 00010: val_loss improved from 0.00010 to 0.00007, saving model to fruitdetect_model.h5\n",
      "2/2 [==============================] - 1s 972ms/step - loss: 4.9327e-05 - accuracy: 1.0000 - val_loss: 7.1522e-05 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9607e-05 - accuracy: 1.0000\n",
      "Epoch 00011: val_loss did not improve from 0.00007\n",
      "2/2 [==============================] - 1s 379ms/step - loss: 2.9607e-05 - accuracy: 1.0000 - val_loss: 7.4278e-05 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0661e-05 - accuracy: 1.0000\n",
      "Epoch 00012: val_loss improved from 0.00007 to 0.00005, saving model to fruitdetect_model.h5\n",
      "2/2 [==============================] - 1s 1s/step - loss: 3.0661e-05 - accuracy: 1.0000 - val_loss: 4.5798e-05 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5467e-05 - accuracy: 1.0000\n",
      "Epoch 00013: val_loss improved from 0.00005 to 0.00004, saving model to fruitdetect_model.h5\n",
      "2/2 [==============================] - 1s 918ms/step - loss: 2.5467e-05 - accuracy: 1.0000 - val_loss: 4.3600e-05 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.7297e-05 - accuracy: 1.0000\n",
      "Epoch 00014: val_loss improved from 0.00004 to 0.00003, saving model to fruitdetect_model.h5\n",
      "2/2 [==============================] - 1s 833ms/step - loss: 1.7297e-05 - accuracy: 1.0000 - val_loss: 3.4788e-05 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.3649e-05 - accuracy: 1.0000\n",
      "Epoch 00015: val_loss improved from 0.00003 to 0.00002, saving model to fruitdetect_model.h5\n",
      "2/2 [==============================] - 1s 935ms/step - loss: 1.3649e-05 - accuracy: 1.0000 - val_loss: 2.1970e-05 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.8335e-05 - accuracy: 1.0000\n",
      "Epoch 00016: val_loss did not improve from 0.00002\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 1.8335e-05 - accuracy: 1.0000 - val_loss: 2.2802e-05 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.4595e-05 - accuracy: 1.0000\n",
      "Epoch 00017: val_loss did not improve from 0.00002\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 1.4595e-05 - accuracy: 1.0000 - val_loss: 3.1221e-05 - val_accuracy: 1.0000\n",
      "End time: 2022-01-23 22:56:21.415369\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(training_set,\n",
    "                    steps_per_epoch=2,\n",
    "                    epochs=20,\n",
    "                    validation_data=test_set,\n",
    "                    validation_steps=2,callbacks=[early_stopping_callback,checkpoint_callback])\n",
    "\n",
    "print(\"End time:\",datetime.datetime.now())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n"
     ]
    }
   ],
   "source": [
    "# Test Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"../data/fruits_cnn_data/Validation/Banana/6_100.jpg\")\n",
    "\n",
    "print(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape original : (100, 100, 3)\n",
      "image shape after resize : (224, 224, 3)\n",
      "[[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"image shape original :\", img.shape)\n",
    "\n",
    "img = cv2.resize(img,(224,224))\n",
    "\n",
    "print(\"image shape after resize :\", img.shape)\n",
    "\n",
    "print(img)\n",
    "# img is a big matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]]]\n"
     ]
    }
   ],
   "source": [
    "# Converting entire image frame into float\n",
    "img = img.astype('float32')\n",
    "img = img/255  # diving will convert image to range 0 to 1 making\n",
    "            # computations easier & faster.\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img.shape\n",
    "print(img)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction probabilities for each class [[1.7939323e-09 9.9997246e-01 2.7520464e-05]]\n",
      "class index dictionary  {'Apple Braeburn': 0, 'Banana': 1, 'Dates': 2}\n",
      "key list  ['Apple Braeburn', 'Banana', 'Dates']\n",
      "predicted class is : Banana\n"
     ]
    }
   ],
   "source": [
    "# Load keras model & predict\n",
    "\n",
    "model = keras.models.load_model(\"./fruitdetect_model.h5\")\n",
    "pred = model.predict(img)\n",
    "print(\"prediction probabilities for each class\", pred)\n",
    "\n",
    "class_detected_max_probs = np.argmax(pred)\n",
    "\n",
    "class_dict = training_set.class_indices\n",
    "print(\"class index dictionary \",class_dict)\n",
    "\n",
    "key_list = list(class_dict.keys())\n",
    "print(\"key list \", key_list)\n",
    "\n",
    "# pass max probability index to get class\n",
    "print(\"predicted class is :\", key_list[class_detected_max_probs])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n",
      "image shape original : (100, 100, 3)\n",
      "image shape after resize : (224, 224, 3)\n",
      "[[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n",
      "[[[[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   ...\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]]]\n",
      "prediction probabilities for each class [[2.2763838e-11 2.3962210e-13 1.0000000e+00]]\n",
      "class index dictionary  {'Apple Braeburn': 0, 'Banana': 1, 'Dates': 2}\n",
      "key list  ['Apple Braeburn', 'Banana', 'Dates']\n",
      "predicted class is : Dates\n"
     ]
    }
   ],
   "source": [
    "# Another Prediction\n",
    "\n",
    "# Import\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# REad test image\n",
    "img = cv2.imread(\"../data/fruits_cnn_data/Validation/Dates/14_100.jpg\")\n",
    "print(img)\n",
    "\n",
    "# Check shape & resize\n",
    "print(\"image shape original :\", img.shape)\n",
    "img = cv2.resize(img,(224,224))\n",
    "print(\"image shape after resize :\", img.shape)\n",
    "print(img)\n",
    "\n",
    "# Converting entire image frame into float\n",
    "img = img.astype('float32')\n",
    "img = img/255  # diving will convert image to range 0 to 1 making\n",
    "            # computations easier & faster.\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img.shape\n",
    "print(img)\n",
    "\n",
    "# Predict\n",
    "pred = model.predict(img)\n",
    "print(\"prediction probabilities for each class\", pred)\n",
    "\n",
    "class_detected_max_probs = np.argmax(pred)\n",
    "\n",
    "class_dict = training_set.class_indices\n",
    "print(\"class index dictionary \",class_dict)\n",
    "\n",
    "key_list = list(class_dict.keys())\n",
    "print(\"key list \", key_list)\n",
    "\n",
    "# pass max probability index to get class\n",
    "print(\"predicted class is :\", key_list[class_detected_max_probs])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}